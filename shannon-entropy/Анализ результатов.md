В таблице представлены полученные оценки энтропии $H_1$ (по однобуквенным частотам), $H_2$ (по частотам пар, делённая на 2) и $H_3$ (по частотам троек, делённая на 3) для трёх сгенерированных файлов, а также **максимально возможная энтропия** для алфавита каждого файла и **теоретическая энтропия** для файлов 1 и 2 (исходя из заданных распределений).

| Название файла | $H_1$  | $H_2$  | $H_3$  | Максимально возможное значение энтропии | Теоретическое значение энтропии |
| -------------- | ------ | ------ | ------ | --------------------------------------- | ------------------------------- |
| `file1.txt`    | 2.0000 | 1.9998 | 1.9991 | 2.0000                                  | 2.0000                          |
| `file2.txt`    | 1.7523 | 1.7521 | 1.7515 | 2.0000                                  | 1.7427                          |
| `file3.txt`    | 4.0182 | 3.5701 | 3.1365 | 4.7549                                  | –                               |

## Объяснение результатов

##### Файл 1 (равномерное распределение 4 символов)
Теоретическая энтропия равна $log_2 4 = 2$ бит/символ, что совпадает с максимально возможной. Оценка $H_1$ в точности равна 2.0000 – это ожидаемо, так как частоты символов в достаточно длинной последовательности (18 000 символов) практически совпадают с вероятностями. Оценки по парам и тройкам $(H_2$ и $H_3$) также очень близки к 2 (отклонения менее 0.001), что свидетельствует об отсутствии статистических связей между соседними символами – символы генерировались независимо.

##### Файл 2 (неравномерное распределение: вероятности 0.5, 0.25, 0.15, 0.1)  
Теоретическое значение энтропии, рассчитанное по формуле Шеннона:
$$
H = -\sum_{i=1}^{4} p_i \log_2 p_i \approx 1.7427\ \text{бит/символ}.
$$
Полученная оценка $H_1 = 1.7523$ несколько выше теории (отличие ≈0.01). Это объясняется статистическими флуктуациями частот в конечной выборке: при длине 18 000 символов выборочные частоты могут немного отклоняться от истинных вероятностей, что приводит к небольшому завышению оценки энтропии (смещение оценки максимального правдоподобия). Оценки $H_2$ и $H_3$ также близки к $H_1$ (различия менее 0.001), что опять же подтверждает независимость символов – для независимого источника энтропия на символ, вычисленная по блокам любой длины, должна совпадать с однобуквенной энтропией (в пределе бесконечной длины). Наблюдаемые малые различия – следствие конечности выборки.

##### Файл 3 (обработанный английский текст, алфавит 26 букв + пробел)  
Максимально возможная энтропия для такого алфавита равна $log_2 27 \approx 4.7549$ бит/символ. Реальная энтропия языка существенно ниже из-за неравномерности частот букв и наличия межсимвольных зависимостей.

$H_1 = 4.0182$ отражает энтропию, обусловленную только частотным распределением отдельных букв (и пробела). Это значение близко к известным оценкам энтропии английского языка для однобуквенного распределения (около 4.1–4.2 бит).

$H_2 = 3.5701$ заметно меньше $H_1$. Это означает, что учёт пар соседних символов (с перекрытием) снижает неопределённость: зная предыдущую букву, следующую предсказать легче. Другими словами, в тексте существуют статистические связи между соседними символами.

$H_3 = 3.1365$ ещё ниже – при учёте контекста из двух предыдущих символов предсказуемость возрастает. Уменьшение энтропии с ростом длины блока характерно для естественных языков и отражает их избыточность.

---

Таким образом, все результаты качественно и количественно согласуются с теоретическими представлениями: для независимых источников оценки по блокам совпадают с однобуквенной энтропией (в пределах статистической погрешности), а для реального текста учёт контекста даёт закономерное снижение энтропии. Программа работает корректно и позволяет наглядно продемонстрировать эти эффекты.
